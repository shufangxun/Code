### 2019.09.15



[TOC]

#### 1. **NMS & IOU 代码**
    ```python
    ## 获得顺序
    order = scores.argsort()[::-1]
    ...
    while order.size > 0:
        i = order[0]
        keep.append(i)
        ...
        ## 比较
        np.maximum(x1[i], x1[order[1:]])
        ...
        detlist = np.where(iou <= thr)[0]
        order = order[detlist + 1]

    ```


#### 2. **直方图均衡**  
   ```python
   ## 图片数组转换
   img = np.asarray(img)
   img = Image.fromarray(img)

   ## 统计灰度级别
   numpix = np.zeros([256])
   ...
   numpix[img[i][j]] += 1

   ## 累积归一化
   sumpix = np.zeros([256])
   ...
   sumpix[i] += sum(propix[:i])

   ## 重新映射  
   new_img = np.empty(img.shape, dtype=np.uint8)
   ...
   new_img[i][j] = 256 * sumpix[img[i][j]]

   ``` 



#### 3. **Guide Achoring**  
**是什么？**    

网络通过训练自动生成 anchor, 包括中心点 x,y 和 长宽 w,h, 无需预设  

**为什么？**   

因为预设的 anchor 尺寸和纵横比是固定的，无法适应极端情况，并且离散的尺寸和纵横比不利于学习，因此考虑让网络自己学习anchor  

**怎么做？**

1. 基于**条件分布**   
    $$p(x,y,w,h|I) = p(x,y|I)p(w,h|x,y,I)$$   

2. **先确定中心点，再预测长宽**  

    - **确定中心点**  
    将feature map 通过 1 x 1 conv + sigmoid, 得到 w x h x 1 的中心点坐标得分

    - **确定长宽**  
    将feature map 通过另一个 1 x 1 conv， 得到 w x h x 2 的长宽dw, dh, 后续需要处理，因为 anchor 是相对于原始图片的:  

        $$
        w = \sigma * s * \exp(dw); h = \sigma * s * \exp(dh); 
        $$    
通过上面两步可以得到 **anchor**  

3. **特征适配**  
    传统情况需要做 ROIpooling 或者 ROIalign 进行尺度统一，但 GA 的 anchor 尺寸在各个位置都不一样，需要调整获得新的**特征图**   

    具体做法是将dw, dh 通过 1 x 1 卷积学习到信息给DeforConv  
    ```python
    offset_channels = kernel_size * kernel_size * 2
    self.conv_offset = nn.Conv2d(
        2, deformable_groups * offset_channels, 1, bias=False)
    ```
**训练**  
- 损失函数加入坐标和长宽loss
- anchor位置训练: feature map 分为物体中心区域(CR), 忽略区域(IR)和外围区域(OR)，将 ground truth 对应在 feature map 上的区域标为物体中心区域，在训练的时候作为正样本，其余区域按照离中心的距离标为忽略或者负样本, FPN中多尺度，优先级 CR > IR > OR
- anchor 尺寸训练
采样了9组尺寸，选取其中最大IoU的尺寸


#### 4. **反向传播 & softmaxloss & 矩阵求导**  
   链式法则和维度匹配


#### 5. **mAP 的理解**
- 定义为PR曲线下的面积，综合Precision和Recall两个指标，评价模型优劣
- 遍历每一个点计算当前Recall下的Precision，注意TP只有一个，FP包含小于阈值和GT的多余框
 - COCO评估在不同的IoU[0.5:0.05:0.95]共10个IoU下的AP，并且在最后以这些阈值下的AP平均作为结果，记为mAP@[0.5, 0.95]
- VOC只评测了IOU=0.5下的AP值。因此相比VOC而言，COCO数据集的评测会更加全面：不仅评估到物体检测模型的分类能力，同时也能体现出检测模型的定位能力



#### 6. **Batch Norm**  
**训练和测试时的区别，以及如何在测试时加速**    
1. 训练的时候是以batch操作，计算方差和均值并滑动平均保留，学习两个参数$\gamma$和$\beta$，而测试阶段是一个样本，用保存的方差和均值测试  
2. 将Conv和BN融合，对卷积核进行一定的缩放，进行加速 
    [链接](https://zhuanlan.zhihu.com/p/48005099)

**为什么能起作用，解决了什么问题**
1. 解决了梯度消失问题
       层与层之间是关联的，但是每一层的数据分布是不一样的，这导致训练困难；并且反向传播过程中梯度的微小改变都将造成蝴蝶效应，所以运用BN将数据分布归一化，加快训练
2. 正则化作用
3. 学习率可以调大  

**不足之处** 

依赖于batchsize，当batchsize过小时，由于方差和均值都是训练阶段基于batchsize得到的（虽然可以滑动平均），不能体现数据的真实分布，导致训练，测试不统一



#### 7. **梯度下降算法VS拟牛顿法**  
- 梯度下降法是一阶导，拟牛顿法是二阶导
- 神经网络是大数据，高维度，非凸的，梯度下降法相比牛顿法更合适
    - 大数据中都是Batch处理，计算梯度（**一阶导**）引入了噪声，牛顿法（**二阶导**）噪声更大
    - 高维下Hessian矩阵计算复杂，而梯度下降是稀疏计算
    - 非凸时Hessian矩阵非正定，容易陷入**鞍点**
- 鞍点  
鞍点是梯度为0，但既不是极大值点也不是极小值点的临界点

#### 8. **解决创建环境时conda版本不匹配问题**  
`./bashrc`和` .bash_profile`区别：  
`./bashrc`每次bash shell被打开时,该文件被读取，` .bash_profile`登录时,该文件仅仅执行一次
```Shell
# 查看 .bashrc 和 .bash_profile
ls -a
# 查看文件大小  
du -h/-b
# 更改后生效
source ~/.bashrc
```

### **2019.09.16**
#### 1. 论文阅读

**《Optical spectrum feature analysis and recognition for optical network security with machine learning》**  

**摘要**  

论文针对侵入信号这一攻击类型，采用支持向量机和一维卷积，分析频谱数据，检测攻击，准确率达到98.54% 和 100% 

**三种攻击**  

窃听(eavesdropping)  
伪装攻击(masquerade attack):伪装成用户获得权限  
干扰攻击(jamming attack):阻止网络工作

**主体思想**

**网络状态隐藏在光学层参数中，** 因此可以通过监视和分析物理层参数和性能来检测物理层攻击。 通过监测物理层参数（本文利用光谱），获得物理层损耗和参数，最后对网络安全环境进行分析，实现未授权信号的检测和识别。  


#### 2.mmdetection相关  
**目标检测学习率设置**  
常用计算方法是：lr = 0.02 / 8 x num_gpus x img_per_gpu / 2

**模型权重加载优先级**  
resume_from > load_from > pretrained   


### **2019.09.17**
#### 1. 决策树
**是什么**？

基于特征对数据进行分类的模型，每次选取**单一特征**进行，可以看作是if-then规则的集合，主要流程分为三个：特征选择，决策树生成，决策树剪枝

**怎么做？**  

- 特征选择  

  ID3: 以信息增益最大化为标准，$G(D,A) = H(D) - H(D|A)$
  
  C4.5: 以信息增益比最大化为标准
  
  CART：分类树以基尼系数最小为标准，回归树以误差平方和最小为标准
  
- 决策树生成

  将所有数据放在根节点，根据评价指标选取特征，直到没有特征可选

- 决策树剪枝

  决策树倾向于正确分类当前数据，泛化能力因此会变弱，剪枝就是为了减轻**过拟合问题**，自底向上处理叶子结点

**为什么?**

- 为什么要信息增益最大

  $H(D|A)$ 是以特征$A$分类时样本的不确定性，为了更好地分类，希望选取$A$特征之后样本的不确定性能够尽可能小，所以采用信息增益最大化

- 为什么信息增益为指标倾向于取值较多地特征

  原因同上

#### 2. FPN中anchor策略和ROIPooling方法　[链接1](https://blog.csdn.net/u012426298/article/details/81516213)  [链接2](https://www.cnblogs.com/hellcat/p/9741213.html)

**是什么**？

目标检测中ROIpooling只用一个尺度的feature map，这导致小目标的ROI很有可能已经消失，FPN提出多尺度feature map，用于ROIpooling，改善了小目标检测性能

**网络结构**

- 自底向上

  网络的正向传播，feature map 逐渐变小，分为$\{$C1, C2, C3, C4, C5$\}$，大小依次减小
  
- 自顶向下

  1. 首先有个细节，$C5$ 首先经过1x1 卷积降维到256，记为$P5$，也作为一个feature map

  2. 横向连接＋上采样，$P5$上采样两倍和C4逐元素相加，产生$P4$，$P3$，$P2$，注意**维度都要保持一致**

  3. 然后对生成的$\{$P5, P4, P3, P2$\}$都要经过一个 3x3 SAME卷积，**去除上采样的混叠影响**
  
  4. $P5$ 做最大池化生成$P6$用于RPN，生成ROI，不用于ROIpooling

**anchor策略**

每个feature map用一种尺度的anchor，但是长宽比还是三种，$\{$P6, P5, P4, P3, P2$\}$对应$\{$512^2^, 256^2^, 128^2^, 64^2^, 32^2^$\}$，注意到**更深的feature map对应更大的anchor**，因为感受野更大，检测更大尺度



**ROIpooling策略**

每个feature map做pooling，ROI根据面积映射公式对应到各自的feature层级，公式如下：
$$
k = [k_{0}+ log_{2}\frac{\sqrt{wh}}{224}]
$$
其中$k~0~=4$，代表层级，$wh$代表ROI面积，面积越大，层级越深

**问题**

1. 如何确定某个 ROI 使用哪一层特征图进行 ROIpooling ?

   公式映射


2. 为什么要所有特征图维度是256?  

   为了参数共享，这样所有层级共享分类层，端到端训练

3. 每个feature map的anchor尺度单一?

   因为有多个feature map，而且anchor是密集滑动

4. 横向连接的作用?  
   
   融合低层位置信息，高分辨率


### **2019.09.18**

#### 1.百面机器学习

**特征工程**

- 特征归一化的目的

  为了统一量纲，有利于训练，比如梯度下降，注意这对决策树无效，因为是以信息增益为指标

- 数据不足的处理方式

  数据不足会导致过拟合，所以需要正则化，数据增强，dropout，模型集成

**降维**

- 为什么需要降维

  数据在高维冗余，如三维空间上的一个平面，在合适的坐标变化下可以在二维空间上清晰表示，所以需要降到低维。

- 降维和特征提取的区别

  降维是在低维生成新的有代表性的特征；特征提取是按照重要性选择特征

- PCA介绍

  无监督降维，基于最大方差(最小投影距离)，要先**数据归一化**，然后计算协方差，求协方差的特征值和特征向量，选取占比最大的几个  

- LDA介绍

  有监督降维，基于类间方差最大和类内方差最小，解决PCA的类别划分不清楚问题

- PCA和LDA应用场景  

  PCA是无监督降维，不会考虑类别，而LDA是考虑类别的，比如在语言识别时，先用PCA去掉固定的噪声，然后用LDA，区分每一个人的声音

**采样**

- 采样的目的

  以少量的数据表征真实分布，比如训练集就是为了在训练时表征真实样本情况

- 正负样本不平衡时如何采样

  正负样本不均衡会导致训练和测试时数据分布不一，影响模型性能，可以采用过采样和欠采样。过采样是对少量样本复制，这会导致过拟合，欠采样是丢弃多数样本，这会丢失有用信息

- 改进的采样

  Smote，Balance Cascade

**模型评估**

- 为何要模型评估

  只有选择与问题相匹配的方法，才能快速发现模型的不足，迭代更新

- 准确率、精确率、召回率

  **准确率** ：正确分类的样本占总样本的比例，在不同类样本不平衡时，模型倾向于分类到多数样本上，因为这样做损失小，在类别不平衡时，采用平均类别准确率  

  **精确率**：TP / (TP+FP)

  **召回率**：TP / (TP+FN)
  
- Top N

  将置信度前N的样本判断为正样本，计算Precision和Recall

- ROC 和 P-R曲线

  ROC对正负样本不均衡稳定，因为是基于TPR和FPR

- AUC的计算

  复杂度低的方法要会

- 过拟合和欠拟合


 ### **2019.09.20** 

 #### 1.空洞卷积

空洞卷积的为什么能增大感受野？为什么feature map不变小？优点？ 

**卷积核空洞**，变相增大了卷积核

- 不做池化，**维持高分辨率**（feature map不变）的同时增大感受野
- 没有**增加额外参数**，实际上卷积核还是3 x 3，只是卷积时每隔rate-1个像素点，未被计算的像素点权重为0

空洞卷积有什么问题？

**棋盘效应**，卷积后相邻像素点相关性不足

- 因为是空洞卷积的权重是稀疏的，这导致相邻像素点相关性不足，空洞率很大时，有效权重退化为中间一个点，无法捕获全局信息
- 解决方案是叠加多个空洞率的卷积+全局平均池化 

#### 2.感受野的计算

$$
R~n+1~ = R~n~ + (K~n+1~-1)\coprod_{i=1}^{n}S~i~
$$

其中$R~n~$是感受野尺寸，$K~n~$是卷积核尺寸，$S$是滑动步长

#### 3.DeeplabV3 ASPP 

**concate** : 1 x 1卷积 + 空洞率[6,12,18]的 3 x 3卷积 + GAP 后再 1 x 1卷积降维到256 

- 1 x 1 + BN:（512，h，w）-> (256, h, w)
- 3 x 3 [rate=6] + BN:（512，h，w）-> (256, h, w)
- 3 x 3 [rate=12] + BN:（512，h，w）-> (256, h, w)
- 3 x 3 [rate=18] + BN:（512，h，w）-> (256, h, w)
- GAP: （512，h，w）-> (512, 1, 1) 
- 1 x 1 + BN:（512，1，1）-> (256, 1, 1)
- upsample: （256，1，1）-> (256, h, w)concate + 1 x 1 + BN:  (1280, h, w) -> (256, h, w)

**代码**

```python
feature_map_h = feature_map.size()[2] # (== h/16)
feature_map_w = feature_map.size()[3] # (== w/16)
out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))
out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))
out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))
out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))

out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))
out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))
out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode="bilinear") # (shape: (batch_size, 256, h/16, w/16))

out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))
out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))
out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))
```

**为什么要用全局平均池化？**

因为随着dilate rate变大，卷积核的有效权重变小，当rate=kernel size时，只剩中间的权重，退化为1x1卷积，没有捕获上下文信息

#### 4.crossentropyloss，softmaxloss, softmax 梯度推导

#### 5.ROIpooling & ROIAlign 

ROIpooling 

经历两次量化，且都是最邻近插值，浮点型变为整型，会丢失精度

- 第一次量化

  将ROI映射到feature map对应的位置，**尺寸不是整数**，量化一下，设为S

- 第二次量化

  当要pooling成对应的bins时，每个bins尺寸不是整数，量化一下

- maxpooling 

ROI Align

不进行量化，而是通过双线性插值操作，保留了空间位置

- 双线性插值是两个方向单线性插值的结合，可以先单线性找规律


#### 6.MobileNet V1 & V2

V1: 深度可分离卷积 + ReLU6 [ min(max(0,x),6) ]

V2: Inverse Residual，先升维度再深度可分离卷积，再接线性函数输出