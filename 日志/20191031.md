# 1. 深度学习

## 1.1 Diceloss

Diceloss 
应用于图像分割，直接用来优化评价指标，实际上主要还是靠BCE优化，可以加权到BCE中
$$
Diceloss = 1 - \frac {2|X \cap Y|}{|X| + |Y|}
$$
分子可以两个矩阵点乘后统计1数目等价，分母分别统计两个矩阵中1数目

```python
def forward(self, output, target):
        inter = 2 * (torch.sum(output * target) + self.smooth)
        union = torch.sum(output) + torch.sum(target) + self.smooth + self.eps
        return 1 - inter / union
```

## 1.2 优化器总结

## 1.3 NMS相关

## 1.4 ResNexth和分组卷积

**范式**： split-transform-merge  
**结构**：分组卷积

- 1 x 1卷积将128通道分为32个4通道
- 对4通道分别做3x3卷积操作  分支同构
- 再聚合做1x1卷积

**优势**

- 参数量未变下效果更好，代价是运行时间久一点(过多的分组卷积操作会增大MAC，从而使模型速度变慢)
- 分支同构，模型简单

**分组卷积有效性分析**

- 卷积未出现之时，是全连接操作，每一个神经元都和前面所有神经元有关
- 后来发现图像的局部相关性，利用卷积－局部相关和权重共享，在三个维度上操作
    1. **所有channel**
    2. 局部ｗ和ｈ
- 卷积对所有通道操作，也可能是一种信息浪费。不同的卷积参数会产生不同的卷积效果，因而在不同的通道中，最终的输出结果也有所不同
- 因此考虑通道的局部相关性，将输入数据channel分为多组，分别做卷积，再拼接

## 1.5 Flops和parameters

**定义**  
Flops: floating point operations，指浮点运算数，可作为计算量，与输入的Feature map有关  
Parameters: 卷积层的参数，与输入无关

**Flops**计算
> n个数相加计算量n-1  
> n个数相乘计算量n

设 $C_{i}$ 和 $C_{o}$ 分别是输入输出channel，$HW$是输出层Feature map，$K$是卷积核尺寸，卷积核Flops:
$$
(2 \cdot C_{i} \cdot K^{2} - 1) \cdot HW \cdot C_{o}
$$

可以分为两步， $(2 \cdot C_{i} \cdot K^{2} - 1) =  (C_{i} \cdot K^{2}) +  (C_{i} \cdot K^{2} - 1)$ ，第一项是乘法运算数，第二项是加法运算数，因为n个数相加，要加n-1次，所以不考虑bias，会有一个-1；如果考虑bias，刚好中和掉，括号内变为 $(2 \cdot C_{i} \cdot K^{2} )$

全连接层Flops：$(2 \cdot I - 1 )\cdot O$

**Parameters**　　
就是简单的卷积核参数计算$C_{i}, C_{o}, K$　　
$$
Para = C_{i} \cdot K^{2} \cdot C_{o}
$$

**Flops和Parameter关系**  
参数和输入无关，Flops和参数有关系
