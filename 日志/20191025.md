# **2019.10.25**

## **1. 论文阅读**

- [x] Mask R-CNN
- [x] FastSWA
- [ ] Segsort
  
## **2. 机器学习**  

### **PCA与SVD关系** [参考](cnblogs.com/pinard/p/6239403.html)

**PCA**  
PCA属于**无监督降维**，找出数据里最主要成分去代替原始数据。具体的，设数据集是n维的，共有$m$个数据$(x(1),x(2),...,x(m))$。希望将这m个数据的维度从$n$维降到$k$维，并且新数据集尽可能的代表原始数据集。

方法  
基于最大投影方差或者最小投影距离

流程

- 数据归一化
- 计算样本协方差$XX^{T}$
- 求协方差特征值，取出前n个最大的特征向量并标准化，组成特征矩阵W
- 将原数据每一个样本通过W变换为新样本 $mn*nk = mk$

优点

- 仅以方差衡量信息量，不受其他因素影响
- 各主成分之间正交，消除原始数据成分间的相互影响的因素
- 主要运算是特征值分解，易于实现

缺点

- 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。
- 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

PCA与SVD的关系和区别

- SVD是$mn = mm * mn * nn$，是两个方向上主成分，而PCA只能获得单个方向上的主成分，而PCA仅用SVD的右奇异矩阵
- SVD左奇异矩阵可以用于行数的压缩，右奇异矩阵可以用于列数压缩
- SVD有个好处，有一些实现算法可以不求先求出协方差矩阵，也能求出右奇异矩阵。也就是说，PCA可以不用做特征分解，而是做SVD来完成。这在样本量很大的时候很有效。

### **Softmax和多个Logstic区别**

普通的logstic回归是二分类问题，而Softmax是多分类问题，要想实现多分类，需要改进logistic回归，具体做法是：

- 每个类别都建立一个二分类器，属于该类别的为1，其他为０
- 对于选择softmax还是k个logistics回归，取决于所有类别之间是否互斥。
  - 所有类别之间明显互斥用Softmax；
  - 所有类别之间不互斥有交叉的情况下最好用k个logistics分类器。



### **EM算法**

### **K means**

## **3. 概率论和矩阵**

### **雅可比矩阵**

- 雅可比矩阵是多元函数一阶偏导数的集合，对应一个**函数族**，有m个函数，每个函数有n个变量  

- 由于矩阵描述了向量空间中的运动——变换，而雅可比矩阵看作是将点{${x_{1},x_{2}...x_{n}}$}转化到点{${y_{1},y_{2}...y_{m}}$}，或者说是从一个n维的欧式空间转换到m维的欧氏空间

### **贝叶斯派 vs 频率派**

- 先验分布 & 后验分布
- 最大似然和最大后验
- 贝叶斯公式
  
## **4. 刷题**
