### **2019.10.15**



- [**2019.10.15**](#20191015)
- [**1. warmup**](#1-warmup)
- [**2. Gradual Warmup**](#2-Gradual-Warmup)
- [**3. label smoothing**](#3-label-smoothing)
- [**4. 知识蒸馏**](#4-知识蒸馏)

### **1. warmup**  
**是什么**  

warmup是一种学习率预热的方法，训练开始时先使用一个小的学习率，训练一些epoch，再达到一定效果后修改为预先设置的学习率来进行训练

**为什么**   

训练初始阶段，模型的权重(weights)是随机初始化的，此时选择一个较大学习率,可能带来模型的不稳定(振荡)，warmup使得开始的几个epoch内学习率较小，模型可以慢慢趋于稳定，等模型相对稳定后再选择预先设置的学习率进行训练，使得模型收敛速度变得更快，模型效果更佳。

另一种解释
- 在训练的开始阶段，模型权重迅速改变  
  训练初始阶段，模型对数据的分布“理解”为零，在初始训练中，模型会快速对数据进行学习，若学习率很大，权重会改变很大从而过拟合，需要后续几个epoch来拉回

- mini-batch size较小，样本方差较大  
  在训练的过程中，如果mini-batch内的数据分布方差特别大，会导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓

**改进 — Gradual Warmup**   

从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。Facebook提出gradual warmup来解决，即从最初的小学习率开始，每个step增大一点，直到达到最初设置大的学习率时，采用最初设置的学习率进行训练。


**代码**
```python
def get_lr(self):

    if self.last_epoch > self.total_epoch:
        if self.after_scheduler:
            if not self.finished:
                self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]
                self.finished = True
            return self.after_scheduler.get_lr()
        return [base_lr * self.multiplier for base_lr in self.base_lrs]

    return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]
```


### **2. Gradual Warmup**
>《Accurate, Large Minibatch SGD:
Training ImageNet in 1 Hour》  

**目标**    
以大mini batch训练网络  

**线性准则**  
mini batch 以k倍增长，等效于学习率以k倍增长

**两个特殊情况**
- 在训练的开始阶段，模型权重迅速改变 
- mini-batch size较小，样本方差较大  

引入 gradual warmup

constan warmup：更适合目标检测和语义分割这类微调所有层的领域 

**实现细节**  

weight decay：BN层不做weight decay  
initlization：卷积层用kaiming init，全连接层用$\mu=0,\sigma^{2}=0.01$的高斯分布，BN层的$\gamma=1$，但每一个残差块的最后一个BN层$\gamma=0$，加快了开始的训练


### **3. label smoothing**

**思想**  [参考](https://zhuanlan.zhihu.com/p/72416162)

不同于传统多分类问题中使用确定的标签作为硬目标，标签平滑使用硬目标的加权平均和标签的均匀分布作为**软目标**，属于一种正则化，是对数据的预处理。

**代码**    
```python  
def smooth_labels(y, smooth_factor=0.1):
    assert len(y.shape) == 2
    if 0 <= smooth_factor <= 1:
        # label smoothing ref: https://www.robots.ox.ac.uk/~vgg/rg/papers/reinception.pdf
        y *= 1 - smooth_factor
        y += smooth_factor / y.shape[1]
    else:
        raise Exception(
            'Invalid label smoothing factor: ' + str(smooth_factor))
    return y
```

### **4. 知识蒸馏**
《Distilling the Knowledge in a Neural Network》
  
[参考1](https://xmfbit.github.io/2018/06/07/knowledge-distilling/)
[参考2](https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230031-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8FKnowledge-Distillation/D%230031.md)
[参考3](https://www.zhihu.com/question/50519680/answer/136363665)

**思想**   
> 蝴蝶以毛毛虫的形式吃树叶积攒能量逐渐成长，最后变换成蝴蝶这一终极形态来完成繁殖。
 
知识蒸馏，本质上是完成毛毛虫到蝴蝶的转变，因为训练和部署的模型往往是相同的，所以使用同态模型，既会导致模型不能针对特定性任务进行快速训练，在部署时开销又很大。  

一句话概括，知识蒸馏是**弥补分类标签监督信息的不足**，"打散"原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布  

**算法**  

将大网络学习出来的知识作为先验，将先验知识传到小网络中，之后实际应用中部署小规模的神经网络。这样做有三点依据：
- 大网络得到的类别预测包含了数据结构间的相似性；
- 有了先验的小模型只需要很少的新场景数据就能够收敛；
- Softmax函数随着温度变量T的升高分布更均匀。


**数据结构的相似性**   
神经网络在预测分类结果时，往往利用softmax函数产生概率分布，这里加入超参数T来软化分布：
$$
q_{i} = \frac{\exp({z_{i}/T)}}{\sum \exp(z_{i}/T)}
$$

T为温度，当T=1时，是普通的softmax，当T>1时，是将大模型的预测结果软化为小模型的标签

解释：在训练大模型时，正确类别的置信度能够达到0.9，错误类的置信度可能分布在$10^{-8}$ ~ $10^{-3}$这个区间，虽然置信度很小，但是$10^{-3}$还是比$10^{-8}$高了五个数量级，表明与正确类别有结构相似性，需要软化标签来表征这种相似性

**将大模型的soft target作为训练目标**  
同一个样本，用大网络产生的软标签来训练一个小网络时，因为包含了数据结构相似性，**收敛会更快**，可以使用更大的学习率和更少的数据，甚至可以使用无标注的数据来训练小网络    
这个做法类似学习了样本空间嵌入（embedding）信息，从而利用空间嵌入信息学习新的网络。

**随着温度上升，软目标分布更均匀**  
T参数是一个温度超参数，按照softmax的分布来看，随着T参数的增大，这个软目标的分布更加均匀。

**损失函数的设计**  
损失函数是一个加权损失函数，一个是小模型的输出和硬标签的loss，一个是小模型的输出与大模型的软标签loss，注意到这里的小模型输出和大模型软标签都是有温度参数T，所以尺度是$1/T^{2}$，需要加一个系数平衡$T^{2}$和α

证明梯度尺度是$1/T^{2}$：~

**具体步骤**

第一步：训练一个大的模型，让它在训练集上性能良好（有一个好的大模型是知识蒸馏的前提）

第二步：用这个大模型来训练小模型  
在训练时，大模型的倒数第二层先除以一个温度T，然后通过Softmax预测一个软目标，小模型也一样，倒数第二层除以同样的温度T，然后通过Softmax预测一个结果，再把这个结果和软目标的交叉熵作为训练的total loss的一部分。total loss的另一部分是正常的小模型输出和真值标签(hard target)的交叉熵。Total loss把这两个损失加权做为训练小模型的最终loss。

第三步，预测小模型  
预测的时候，温度T= 1，也就是按照常规的Softmax输出。

代码过程

第一步，训练一个大的模型，让它在训练集上性能良好（有一个好的大模型是知识蒸馏的前提）；

第二步，用这个大模型来训练小模型。
在训练时，大模型的倒数第二层先除以一个温度T，然后通过Softmax预测一个软目标，小模型也一样，倒数第二层除以同样的温度T，然后通过Softmax预测一个结果，再把这个结果和软目标的交叉熵作为训练的total loss的一部分。total loss的另一部分是正常的小模型输出和真值标签(hard target)的交叉熵。Total loss把这两个损失加权做为训练小模型的最终loss。
第三步，预测小模型。预测的时候，温度T= 1，也就是直接按照常规的Softmax输出就可以了。

**代码**

train

```python 
def train_epoch(train_loader, big_model, small_model, T, criterion, optimizer, epoch, loss_weight=0.2):

    big_model.eval()
    big_model.cuda()
    small_model.train()
    small_model.cuda()
    losses = AverageMeter()
    top1_acc = AverageMeter()


    pbar = tqdm.tqdm(train_loader)
    for inp, class_target in pbar:
    pbar.set_description('loss: %2.4f, acc: %2.1f' % (losses.avg, top1_acc.avg))
    inp = inp.cuda(async=True)
    input_var = torch.autograd.Variable(inp)

    logits_small = small_model(input_var) #type Var
    logits_big = big_model(input_var) #type Var
    soft_logits_small = Softmax()(logits_small * 1.0/T)
    soft_logits_big = Softmax()(logits_big *1.0/T)

    loss_soft = torch.nn.BCELoss().cuda()(soft_logits_small, soft_logits_big)

    #output = logits_small
    class_target = class_target.cuda(async=True)
    class_target_var = torch.autograd.Variable(class_target)
    loss_hard = criterion(logits_small, class_target_var)

    loss = loss_weight * loss_hard + loss_soft
    prec1 = pytorch_accuracy(logits_small.data, class_target)
  

    losses.update(loss.data[0], inp.size(0))
    top1_acc.update(prec1[0][0], inp.size(0))
    #top1_f1.update(f1score1[0], inp.size(0))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

```

val 
```python 
def val_epoch(val_loader, model, criterion): #temperature 1
    model.eval()
    model.cuda()
    losses = AverageMeter()
    top1_acc = AverageMeter()


    targets = np.array([])
    preds = np.array([])
    for i, (inp, target) in enumerate(val_loader):
        inp = inp.cuda(async=True)
        target_cuda = target.cuda(async=True)
        input_var = torch.autograd.Variable(inp, volatile=True)
        target_var = torch.autograd.Variable(target_cuda, volatile=True)


        output = model(input_var)
        loss = criterion(output, target_var)
        prec1 = pytorch_accuracy(output.data, target_cuda)
        targets = np.append(targets, target.cpu().numpy())
        preds = np.append(preds, np.argmax(output.data.cpu().numpy(), axis=1))


        losses.update(loss.data[0], inp.size(0))
        top1_acc.update(prec1[0][0], inp.size(0)
        return losses.avg, top1_acc.avg #top1_f1
```