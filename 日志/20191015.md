### **2019.10.15**

#### **1. warmup**  
**是什么**  

warmup是一种学习率预热的方法，训练开始时先使用一个小的学习率，训练一些epoch，再达到一定效果后修改为预先设置的学习率来进行训练

**为什么**   

训练初始阶段，模型的权重(weights)是随机初始化的，此时选择一个较大学习率,可能带来模型的不稳定(振荡)，warmup使得开始的几个epoch内学习率较小，模型可以慢慢趋于稳定，等模型相对稳定后再选择预先设置的学习率进行训练，使得模型收敛速度变得更快，模型效果更佳。

另一种解释
- 在训练的开始阶段，模型权重迅速改变  
  训练初始阶段，模型对数据的分布“理解”为零，在初始训练中，模型会快速对数据进行学习，若学习率很大，权重会改变很大从而过拟合，需要后续几个epoch来拉回

- mini-batch size较小，样本方差较大  
  在训练的过程中，如果mini-batch内的数据分布方差特别大，会导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓

**改进 — Gradual Warmup**   

从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。Facebook提出gradual warmup来解决，即从最初的小学习率开始，每个step增大一点，直到达到最初设置大的学习率时，采用最初设置的学习率进行训练。


**代码**
```python
def get_lr(self):

    if self.last_epoch > self.total_epoch:
        if self.after_scheduler:
            if not self.finished:
                self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]
                self.finished = True
            return self.after_scheduler.get_lr()
        return [base_lr * self.multiplier for base_lr in self.base_lrs]

    return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]
```


#### **2. Gradual Warmup**
>《Accurate, Large Minibatch SGD:
Training ImageNet in 1 Hour》  

**目标**    
以大mini batch训练网络  

**线性准则**  
mini batch 以k倍增长，等效于学习率以k倍增长

**两个特殊情况**
- 在训练的开始阶段，模型权重迅速改变 
- mini-batch size较小，样本方差较大  

引入 gradual warmup

constan warmup：更适合目标检测和语义分割这类微调所有层的领域 

**实现细节**  

weight decay：BN层不做weight decay  
initlization：卷积层用kaiming init，全连接层用$\mu=0,\sigma^{2}=0.01$的高斯分布，BN层的$\gamma=1$，但每一个残差块的最后一个BN层$\gamma=0$，加快了开始的训练


#### **3. label smoothing**

**思想**  [参考](https://zhuanlan.zhihu.com/p/72416162)

不同于传统多分类问题中使用确定的标签作为硬目标，标签平滑使用硬目标的加权平均和标签的均匀分布作为**软目标**，属于一种正则化，是对数据的预处理。

**代码**    
```python  
def smooth_labels(y, smooth_factor=0.1):
    assert len(y.shape) == 2
    if 0 <= smooth_factor <= 1:
        # label smoothing ref: https://www.robots.ox.ac.uk/~vgg/rg/papers/reinception.pdf
        y *= 1 - smooth_factor
        y += smooth_factor / y.shape[1]
    else:
        raise Exception(
            'Invalid label smoothing factor: ' + str(smooth_factor))
    return y
```

