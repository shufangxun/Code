
### **2019.09.17**

- [**2019.09.17**](#20190917)
  - [1. 决策树](#1-决策树)
  - [2. FPN中anchor策略和ROIPooling方法　链接1 链接2](#2-fpn中anchor策略和roipooling方法链接1-链接2)
#### 1. 决策树
**是什么**？

基于特征对数据进行分类的模型，每次选取**单一特征**进行，可以看作是if-then规则的集合，主要流程分为三个：特征选择，决策树生成，决策树剪枝

**怎么做？**  

- 特征选择  

  ID3: 以信息增益最大化为标准，$G(D,A) = H(D) - H(D|A)$
  
  C4.5: 以信息增益比最大化为标准
  
  CART：分类树以基尼系数最小为标准，回归树以误差平方和最小为标准
  
- 决策树生成

  将所有数据放在根节点，根据评价指标选取特征，直到没有特征可选

- 决策树剪枝

  决策树倾向于正确分类当前数据，泛化能力因此会变弱，剪枝就是为了减轻**过拟合问题**，自底向上处理叶子结点

**为什么?**

- 为什么要信息增益最大

  $H(D|A)$ 是以特征$A$分类时样本的不确定性，为了更好地分类，希望选取$A$特征之后样本的不确定性能够尽可能小，所以采用信息增益最大化

- 为什么信息增益为指标倾向于取值较多地特征

  原因同上

#### 2. FPN中anchor策略和ROIPooling方法　[链接1](https://blog.csdn.net/u012426298/article/details/81516213)  [链接2](https://www.cnblogs.com/hellcat/p/9741213.html)

**是什么**？

目标检测中ROIpooling只用一个尺度的feature map，这导致小目标的ROI很有可能已经消失，FPN提出多尺度feature map，用于ROIpooling，改善了小目标检测性能

**网络结构**

- 自底向上

  网络的正向传播，feature map 逐渐变小，分为{$C1, C2, C3, C4, C5$}，大小依次减小
  
- 自顶向下

  1. 首先有个细节，$C5$ 首先经过1x1 卷积降维到256，记为$P5$，也作为一个feature map

  2. 横向连接＋上采样，$P5$上采样两倍和C4逐元素相加，产生$P4$，$P3$，$P2$，**维度都要保持一致**

  3. 然后对生成的{$P5, P4, P3, P2$}都要经过一个 3x3 SAME卷积，**去除上采样的混叠影响**
  
  4. $P5$ 做最大池化生成$P6$用于RPN，生成ROI，不用于ROIpooling

**anchor策略**

- 每个feature map用一种尺度的anchor  
  
- 长宽比还是三种，{$P6, P5, P4, P3, P2$}对应{$512^2, 256^2, 128^2, 64^2, 32^2$}，**更深的feature map对应更大的anchor**，因为感受野更大，检测更大尺度



**ROIpooling策略**

每个feature map做pooling，ROI根据面积映射公式对应到各自的feature层级，公式如下：
$$
k = [k_{0}+ log_{2}\frac{\sqrt{wh}}{224}]
$$
其中$k~0~=4$，代表层级，$wh$代表ROI面积，面积越大，层级越深

**问题**

1. 如何确定某个 ROI 使用哪一层特征图进行 ROIpooling ?

   公式映射


2. 为什么要所有特征图维度是256?  

   为了参数共享，这样所有层级共享分类层，端到端训练

3. 每个feature map的anchor尺度单一?

   因为有多个feature map，而且anchor是密集滑动

4. 横向连接的作用?  
   
   融合低层位置信息，高分辨率
