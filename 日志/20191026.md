# 2019.10.26

## 1. 论文阅读

- [x] Segsort
- [x] PSPNet

## 2. 机器学习

### 2.1 余弦相似度

**定义**  
用余弦距离度量相似度，与欧式距离不同的是，欧式距离差异体现在绝对值上，余弦距离体现在方向上的相对差异  
$$
cos\theta  = \frac{A^{T}B}{||A||\cdot||B||}
$$
范围是[-1,1]，1代表方向一致，0代表方向正交，-1代表方向相反
  
**与欧式距离的关系**  
对向量归一化，然后利用几何知识，等价于腰长为1的等腰三角形求余弦值  
$$
euc = \sqrt{2-2cos}
$$

**何时用**  
余弦距离关注维度上相对差异，欧式距离关注数值上绝对差异  
如分析用户观影喜好，(0,1)和(1,0)用余弦距离；分析用户活跃度，(2,300)和(3,40)，用欧式距离

### 2.2 SVM  

具体推导见印象笔记

#### SVM损失函数怎么来的

实际数据有少量噪声，一味追求线性分类会导致过拟合，因此引入软间隔，允许部分点不满足约束:
$$
y_{i}(w \cdot x+b) \geq 1
$$

引入Hinge Loss 衡量允许误分类的程度：
$$
\xi = max(0, 1-y_{i}(w \cdot x+b))
$$

优化目标变为：
$$
min \frac{1}{2}||w||^{2} + C\sum_{1}^{N}\xi_{i}
$$
$$
s.t. y_{i}(w \cdot x+b) \geqslant 1- \xi_{i}
$$
$$
\xi_{i} \geq 0
$$

也就是说只有**分类正确且间隔大于1**的Loss为0

**惩罚因子**和**松弛变量**

- 松弛变量是允许误分类的Loss，惩罚因子就是间隔尽量大，同时使误分类的个数尽量小的调和系数
- 惩罚系数变大，松弛变量变小，就是不允许误分类，
- 惩罚系数变小，松弛变量变大，就是允许误分类，
- $C$越小越容易欠拟合，$C$越大越容易过拟合

#### 核函数  

核函数就是一个函数，接收两个变量，这两个变量是低维空间中的变量，而核函数值是将低维空间中的两个向量映射到高维空间后的内积

核函数选择

- 当特征维数 d 超过样本数 m 时 (文本分类问题通常是这种情况), 使用线性核;
- 当特征维数 d 比较小，样本数 m 中等时, 使用RBF核;
- 当特征维数 d 比较小，样本数 m 特别大时, 支持向量机性能通常不如深度神经网络。

#### SVM和logistic异同点  

不同点  
损失函数：SVM是hinge loss，Logistic是对数交叉熵  
输出：SVM是输出类别，Logistic输出概率  
考虑范围：SVM考虑支持向量，Logistic考虑全部数据  （所以SVM比Logistic鲁棒）  

相同点  
都是有监督的分类算法

### 2.3 KNN

本质  
根据距离样本点最近的K个点所属类别判断样本的类别，所以KNN是有监督的分类算法  

算法  
　　1）计算测试数据与各个训练数据之间的距离；  
　　2）按照距离的递增关系进行排序；  
　　3）选取距离最小的K个点；  
　　4）确定前K个点所在类别的出现频率；  
　　5）返回前K个点中出现频率最高的类别作为测试数据的预测分类

## 3. 刷题

### 3.1 反转链表

迭代法

```python
def reverse(head):
    pre = None # 解决只有一个结点情况
    cur = head
    while head:
        tmp = cur.next
        cur.next = pre
        pre = cur
        cur = tmp
    return pre
```

递归法  

```python
def reverse(head):
    if head is None or head.next is None:
        return head

    p = reverse(head.next) # 每一次反转当前节点
    older = head.next # 将当前节点的下一个节点的指针域指向自己，并将自身的指针域指向空
    older.next = head
    head.next = None

    return p
```
