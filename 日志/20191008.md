### 　**2019.10.08** 
- [**2019.10.08**](#20191008)
  - [**1.Relu系列总结**](#1relu系列总结)
  - [**2.权重初始化**](#2权重初始化)
  - [**3.batch size 越大越好么**](#3batch-size-越大越好么)
  - [**4.BN相关细节**](#4bn相关细节)
  - [**5.Resnet深度解读和缺点分析**](#5resnet深度解读和缺点分析)
  - [**6.网络越大越好么**](#6网络越大越好么)

  

####  **1.Relu系列总结**
**激活函数作用**：激活函数为非线性函数，增加网络的拟合能力，优化整个网络

**Sigmoid函数**

最基本的激活函数，求导平滑，导数是$\sigma(x)(1-\sigma(x))$ ，但其缺点也很多

- **梯度消失**       

  梯度最大值是0.25，且只有0位置，其他位置梯度很小，反向传播是链式法则，经过几次传播后，梯度会很小

- **输出不是zero-center**       
Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢

- **幂指数运算时间久**      

   相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省

**Tahn函数**

解决了zero-center问题，梯度最大值是1.0，然而梯度消失和幂指数运算大问题仍然存在


**ReLU**

优点

- 解决梯度消失问题

- 只有max运算，计算速度快

- 具备稀疏性，收敛速度快

缺点

- ReLU的输出不是zero-centered

- Dead ReLU Problem，某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生:

  1. 参数初始化不好，这种情况比较少见

  2.  学习率设置太高，导致在训练过程中参数更新太大，不幸使网络进入这种状态

  解决方案是用Xavier初始化，避免将学习率设置太大或使用adagrad等自动调节learning rate的算法
  

变体

PReLU 和 ELU

####  **2.权重初始化**  

Xavier 

Kaiming 

#### **3.batch size 越大越好么**  

不是，batchsize过大会掉入局部最小值，mini-batch 人为加入噪声，增强了鲁棒性

#### **4.BN相关细节**

**作用**

**为什么说有正则化效果？**

因为训练时每个batch计算均值和方差，引入了不确定性，而测试时是用整体的均值和方差，从而泛化能力更强 

**为什么归一化之后还要学习β和γ去变换？**   

Batch Normalization 是为了防止落入饱和区，但是有一点点饱和也不是坏处，学习β和γ去控制每个层缩放和平移的程度，保证数据一定特点

#### **5.Resnet深度解读和缺点分析**

缺点  
对数据的波动很敏感，容易过拟合

Resnet本质的思考

[链接1](https://zhuanlan.zhihu.com/p/60668529)  
[链接2](https://www.jianshu.com/p/ca6bee9eb888)

#### **6.网络越大越好么**
不好，太大会导致模型退化，但总体而言，使用大模型优于小模型
- 大模型虽然会过拟合，但可以通过正则化解决
- 小模型虽然泛化性能好，但是更难使用梯度下降等局部方法来进行训练：虽然小模型损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。