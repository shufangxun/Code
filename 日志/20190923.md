### 　**2019.09.23** 

- [**2019.09.23**](#20190923)
  - [**1.凸函数**](#1凸函数)
  - [**2.信息熵、交叉熵、相对熵**](#2信息熵交叉熵相对熵)
  - [**3.Python垃圾回收机制**](#3Python垃圾回收机制)
### **1.凸函数**

简单判断

二阶导数大于0   
 **为什么数学概念中将凹陷的函数叫凸函数**？[参考](https://www.zhihu.com/question/20014186)

定义

凸组合的值 < 凸值组合
$$
f\left ( \theta x + (1-\theta x ) \right ) \leq f\left ( \theta x \right )+ f\left ( 1-\theta x \right )
$$




交叉熵损失和均方损失的凸性判断

[参考1](https://www.cnblogs.com/aijianiula/p/9651879.html)
[参考2](https://blog.csdn.net/zhufenghao/article/details/52735750)


###  **2.信息熵、交叉熵、相对熵**

**信息熵**

表征随机变量或者系统不确定性的指标，信息熵越大，不确定性越大
$$
\sum_{k=1}^{N}p_{k}log\frac{1}{p_{k}}
$$

> 根据真实分布，找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，所要付出的**最小努力**（猜题次数、编码长度等）的大小就是信息熵。



**交叉熵**

在大部分情况下，尤其是优化问题，真实分布是难以精确衡量，需要使用非真实分布的样本去拟合，最优情况下交叉熵=信息熵，$p_{k} = q_{k}$
$$
\sum_{k=1}^{N}p_{k}log\frac{1}{q_{k}}
$$
交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布

**相对熵**

也被称为KL散度，用以衡量两个策略的差异，可以用相对熵来衡量这两者之间的差异。即，相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略）

$$
KL(p||q) = H(p,q) - H(q)   
         = \sum_{k=1}^{N}p_{k}log\frac{1}{q_{k}} - \sum_{k=1}^{N}p_{k}log\frac{1}{p_{k}}
$$

特点
- 非对称   
  $KL(P || Q) \neq KL(Q || P)$ 
- 非负  
  $KL(P || Q) > 0$


### **3.Python垃圾回收机制**

策略

**引用计数**为主，**标记清除**和**分代收集**为辅

- 引用计数

  记录该对象当前被引用的次数，每当新的引用指向该对象时，它的引用计数ob_ref加1，每当该对象的引用失效时计数ob_ref减1，一旦对象的引用计数为0，该对象立即被回收

  **缺点**：循环引用，也就是两个对象相互引用，当为None时，ob_ref = 1, 导致内存泄漏

- 标记清除

  首先标记对象（垃圾检测），然后清除垃圾（垃圾回收）

  > 1. 初始所有对象标记为白色，并确定根节点对象（这些对象不会被删除），标记它们为黑色（表示对象有效）。
  >
  > 2. 将有效对象引用的对象标记为灰色（表示对象可达，但它们所引用的对象还没检查），检查完灰色对象引用的对象后，将灰色标记为黑色。
  > 3. 重复直到不存在灰色节点为止。最后白色结点都是需要清除的对象。

- 分代收集

  > 一种典型的以空间换时间的技术，认为对象存在时间越长，越不可能是垃圾，越应该保留

  将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，0代触发将清理所有三代，1代触发会清理1,2代，2代触发后只会清理自己

工作内容

- 为新生成对象分配内存
- 识别垃圾对象
- 清除垃圾对象，回收内存
