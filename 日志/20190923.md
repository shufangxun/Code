### 　**2019.09.23** 

#### **1.凸函数**

简单判断

二阶导数大于0   
 **为什么数学概念中将凹陷的函数叫凸函数**？[参考](https://www.zhihu.com/question/20014186)

定义

凸组合的值 < 凸值组合
$$
f\left ( \theta x + (1-\theta x ) \right ) \leqslant f\left ( \theta x \right )+ f\left ( 1-\theta x \right )
$$



交叉熵损失和均方损失的凸性判断

[参考1](https://www.cnblogs.com/aijianiula/p/9651879.html)
[参考2](https://blog.csdn.net/zhufenghao/article/details/52735750)


####  **2.信息熵、交叉熵、相对熵**

**信息熵**

表征随机变量或者系统不确定性的指标，信息熵越大，不确定性越大
$$
\sum_{k=1}^{N}p_{k}log\frac{1}{p_{k}}
$$

> 根据真实分布，找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，所要付出的**最小努力**（猜题次数、编码长度等）的大小就是信息熵。



**交叉熵**

在大部分情况下，尤其是优化问题，真实分布是难以精确衡量，需要使用非真实分布的样本去拟合，最优情况下交叉熵=信息熵，$p_{k} = q_{k}$
$$
\sum_{k=1}^{N}p_{k}log\frac{1}{q_{k}}
$$
交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布

**相对熵**

衡量两个策略的差异，可以用相对熵来衡量这两者之间的差异。即，相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略）

$$
KL(p||q) = H(p,q) - H(q)   
         = \sum_{k=1}^{N}p_{k}log\frac{1}{q_{k}} - \sum_{k=1}^{N}p_{k}log\frac{1}{p_{k}}
$$