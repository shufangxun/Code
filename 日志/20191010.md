### **2019.10.10**  

#### **1.对图像做FFT变换，可以用相位谱恢复大致图像，幅度谱却不行？**
[讨论](https://www.zhihu.com/question/23718291)  
幅度只包含了能量信息，而相位 $k_{0}+kx-\omega t$ 则包含了

1. where it starts $k_{0}$
2. where it's going $kx$
3. how fast it gets there $\omega t$




#### **2.Anchor本质**  
传统目标检测是金字塔+遍历滑窗，逐尺度逐位置判断有没有目标，anchor预设不同尺度和不同位置的参考框去覆盖所有位置，本质也是**多尺度的滑窗**，只是不需要遍历  

anchor将问题转化为**固定anchor中是否有目标，和目标的偏移是多少**，从而根据目标框修正anchor    

anchor让网络学习到一种推理能力，根据anchor所对应的像素，判断anchor是目标的概率，anchor可能比感受野大，也可能比感受野小：  
- 如果比感受野大，相当于只看到了关心区域（anchor）的一部分（感受野），通过部分判断整体
- 如果比感受野小，那就是看到了比关心区域更大的信息，判断关心的区域是不是目标。
- 所以感受野大于anchor比较好


#### **3.Faster RCNN 复习**

**3.1 RPN前为什么要3x3卷积？** [参考](https://www.zhihu.com/search?type=content&q=faster%20rcnn)   

因为anchor是多尺寸的，而ROI的提取是基于feature map的每一个像素点，所以用3x3卷积更好的结合邻域信息，表征以当前像素点为中心，一定范围内的区域特征的，用1x1卷积感受野不够

**3.2 RPN训练**

从loss函数入手，对于512×62×37的feature map，有 62×37×9~ **20000个anchor**
$$ L({p_{i},t_{i}}) = \frac{1}{N_{cls}}\sum_{i}L_{cls}(p_{i},{p_{i}}^{\star}) + \lambda \frac{1}{N_{reg}}\sum_{i}{p_{i}}^{\star}L_{reg}(t_{i},{t_{i}^{\star}}) $$

cls部分  

$p_{i}$是anchor置信度，${p_{i}^{\star}}$是anchor被分配的标签(0 or 1)，但注意不是2万个anchor都参与运算(正负样本不平衡，负样本过多导致loss被负样本支配)

anchor sample:
- 对于每一个gt，选择与其IOU最高的anchor作为正样本（**因为有时候正样本的IOU会小于0.7**)
- 对于剩下的anchor，从中选择与任意一个gt的IOU>0.7的作为正样本，**正样本的数目不超过128个**
- 随机选择IOU<0.3的作为负样本，负样本和正样本总数保持为256，正样本不足128用负样本补全
- 迭代时每次重新采样256个
  
reg部分  

${p_{i}^{\star}}$当正样本时1，负样本时0，所以只对**正样本回归**，采用smoothL1，$t_{i}$是预测框相对anchor的偏移，${p_{i}}^{\star}$是gt相对anchor的偏移，所以reg学习的是预测框和gt的偏移，anchor是一个中间量.

**$\lambda$的选取**：$N_{cls}$=256是batchsize, $N_{reg}$=2400，所以$\lambda$选取10，达到平衡，实际上对$\lambda$不敏感  

**3.3 RPN生成ROIs**

训练完之后，要生成用于ROIhead操作的ROIs，基本操作：
- 根据anchor和reg信息做位置回归，限定超出图像边界的positive anchors为图像边界（**防止后续roi pooling时proposal超出图像边界**）
- 选取cls score中**正样本的置信度**并排序，提取前pre_nms_topN(e.g. 6000)，即提取修正位置后的positive anchors，然后剔除尺寸太小的，一般是小于16（**因为下采样16倍**）
- 对第一步提取的修正anchors做NMS(**threshold=0.7**)，然后选取前2000个作为ROIs输出


**3.4 ROIhead训练**

ROI sample
RPN会产生大约2000个RoIs，但不是都拿去训练，而是选择128个训练，保证1 : 3: 
- ROIs和gt_bboxes 的IoU>0.5的，选择32个作为正样本
- ROIs和gt_bboxes的IoU在0.1~0.5之间的选择96个作为负样本（困难样本学习思想）
- 负样本会较少，就用负样本重复补全

分类和回归  

因为是多分类，所以背景类loss也包含，但是回归时有一个疑问，是负样本的也做回归么？




