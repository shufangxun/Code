# 1. 深度学习

## 1.1 Softmaxloss [参考](https://www.zhihu.com/search?type=content&q=softmax%E6%8E%A8%E5%AF%BC)

$Softmaxloss = crossEntropy(Softmax(z_{i}))$
设$x_{i}, z_{i}, q_{i}$分别是原始输入、神经元加权、经过Softmax归一化

- $z = wx$
- $q = softmaxz$
- $Loss = CE(q)$

**求导**  链式法则
$$\frac{\partial Loss}{\partial w} = \frac{\partial Loss}{\partial q} \cdot \frac{\partial q}{\partial z}\cdot \frac{\partial z}{\partial w}$$

$\frac{\partial z}{\partial w}$ 已知，重点是求前面两项$\frac{\partial Loss}{\partial q} \cdot \frac{\partial q}{\partial z}$

- $\frac{\partial Loss}{\partial q}$ 是交叉熵函数求导  
交叉熵只有$p_{k}=1$才有损失累加，所以只针对$q_{k}$

$$
\frac{\partial Loss}{\partial q} = \frac{\partial Loss}{\partial q_{k}} = -\frac{1}{q_{k}}
$$

- $\frac{\partial q_{k}}{\partial z_{i}}$ 是Softmax函数求导，需要分类讨论
  - $i = k$
    $$
    \frac{\partial q_{k}}{\partial z_{k}} = \frac{e^{z_{k}}\sum - e^{z_{k}}e^{z_{k}}}{\sum^2} = q_{k} - q_{k}^2
    $$
  
  - $i \neq k$
    $$
    \frac{\partial q_{k}}{\partial z_{i}} = \frac{0\sum - e^{z_{k}}e^{z_{i}}}{\sum^2} = q_{k} q_{i}
    $$

结合两个导数
$$
\frac{\partial Loss}{\partial z_{i}} = q_{i} - p_{i}
$$

所以当用softmaxloss作为损失函数，梯度传播是当标签是1，预测值减去1；标签是0，不变

**数值稳定**  
为了稳定输出，要统一减去最大的值$D$
$$
D = max(z_{1},z_{2}....z_{n})
$$

## 1.2 L1和L2正则化

**正则化的作用**  
Loss后面加上**权重的正则项**，平衡经验风险最小化和结构风险最小化，尽可能采用简单的模型，降低模型复杂度，以此提高模型泛化能力

**L1正则化**  
在Loss后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）

**L2正则化**  
在Loss后边所加正则项为L2范数，加上L2范数得到的解比较平滑，大部分都很小

**L1和L2正则化使用范围**  
只用于卷积层和全连接层，因为这里参数很多，如果用于BN层，参数过少，会丧失功能

**L2正则化和权重衰减的区别**
- L2正则化是在目标函数中直接加上一个正则项，直接修改了优化目标；权重衰减是在训练的每一步结束的时候，对网络中的参数值**裁剪一定的比例**，优化目标是不变的
- 在梯度下降中，L2正则化等价于权重衰减

## 1.3 YOLO总结

见印象笔记: Yolo系列

## 1.4 ShufflenetV1 & V2

见印象笔记: ShuffleNetV1 and V2，ResNext讲分组卷积部分

## 1.5 FFT加速卷积计算

> 1.Fast training of convolutional networks through ffts  
> 2.Fast training
of convolutional networks through ffts

## 1.6 Everybody dance now

# 2. 算法和数据结构

## 2.1 树的遍历  

见数据结构
