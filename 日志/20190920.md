### **2019.09.20** 

- [**2019.09.20**](#20190920)
  - [1.空洞卷积](#1空洞卷积)
  - [2.感受野的计算](#2感受野的计算)
  - [3.DeeplabV3 ASPP](#3deeplabv3-aspp)
  - [4.crossentropyloss，softmaxloss, softmax 梯度推导](#4crossentropylosssoftmaxloss-softmax-梯度推导)
  - [5.ROIpooling & ROIAlign](#5roipooling--roialign)
  - [5.Tensor 转 numpy](#5tensor-转-numpy)
  - [6.Linux相关](#6linux相关)
  - [7.Mobilenet v1 & v2](#7mobilenet-v1--v2)

#### 1.空洞卷积

空洞卷积的为什么能增大感受野？为什么feature map不变小？优点？ 

**卷积核空洞**，变相增大了卷积核

- 不做池化，**维持高分辨率**（feature map不变）的同时增大感受野
  
- 没有**增加额外参数**，实际上卷积核还是3 x 3，只是卷积时每隔rate-1个像素点，未被计算的像素点权重为0

空洞卷积有什么问题？

**棋盘效应**，卷积后相邻像素点相关性不足

- 因为是空洞卷积的权重是稀疏的，这导致相邻像素点相关性不足，空洞率很大时，有效权重退化为中间一个点，无法捕获全局信息
  
- 解决方案是叠加多个空洞率的卷积+全局平均池化 

#### 2.感受野的计算

$$
R_{n+1} = R_{n} + (K_{n+1}-1)\coprod_{i=1}^{n}S_{i}
$$

其中$R_{n}$是感受野尺寸，$K_{n}$是卷积核尺寸，$S$是滑动步长

#### 3.DeeplabV3 ASPP 

**concate** : 1 x 1卷积 + 空洞率[6,12,18]的 3 x 3卷积 + GAP 后再 1 x 1卷积降维到256 

- 1 x 1 + BN:（512，h，w）-> (256, h, w)
- 3 x 3 [rate=6] + BN:（512，h，w）-> (256, h, w)
- 3 x 3 [rate=12] + BN:（512，h，w）-> (256, h, w)
- 3 x 3 [rate=18] + BN:（512，h，w）-> (256, h, w)
- GAP: （512，h，w）-> (512, 1, 1) 
- 1 x 1 + BN:（512，1，1）-> (256, 1, 1)
- upsample: （256，1，1）-> (256, h, w)concate + 1 x 1 + BN:  (1280, h, w) -> (256, h, w)

**代码**

```python
feature_map_h = feature_map.size()[2] # (== h/16)
feature_map_w = feature_map.size()[3] # (== w/16)
out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))
out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))
out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))
out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))

out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))
out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))
out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode="bilinear") # (shape: (batch_size, 256, h/16, w/16))

out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))
out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))
out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))
```

**为什么要用全局平均池化？**

因为随着dilate rate变大，卷积核的有效权重变小，当rate=kernel size时，只剩中间的权重，退化为1x1卷积，没有捕获上下文信息

#### 4.crossentropyloss，softmaxloss, softmax 梯度推导

#### 5.ROIpooling & ROIAlign 

ROIpooling 

经历两次量化，且都是最邻近插值，浮点型变为整型，会丢失精度

- 第一次量化

  将ROI映射到feature map对应的位置，**尺寸不是整数**，量化一下，设为S

- 第二次量化

  当要pooling成对应的bins时，每个bins尺寸不是整数，量化一下

- maxpooling 

ROI Align

不进行量化，而是通过双线性插值操作，保留了空间位置

- 双线性插值是两个方向单线性插值的结合，可以先单线性找规律

#### 5.Tensor 转 numpy
```python
x = x.to(device)
outputs = model(x)
results = outputs.cpu().data.numpy()
```

#### 6.Linux相关

cp复制文件速度  

**结论**： 复制单一文件，如一个压缩文件夹，速度快；多个文件，如一个文件夹里很多文件，速度慢

**分析**：cp命令要把所有的文件地址变量都找到，然后读取内容，然后在另外的文件夹地址下开辟一块内存去储存文件内容。这就涉及到寻址,而寻址就是造成这种“出奇的慢”的原因

压缩文件
```python
    # 压缩：
    tar -zcvf archive_name.tar.gz filename
    # 解压：
    tar -zxvf archive_name.tar.gz
    # 解压缩到指定路径
    tar -zxvf archive_name.tar.gz -C new_dir
```

#### 7.Mobilenet v1 & v2

比较V1和V2  

V1 : 深度可分离卷积 + Relu6  
V2 : Inverse block + linear

Relu对于低维信息丢失大，在高维表现好，所以先升维度再做卷积，最后不用Relu，改为linear