# **2019.10.25**

- [**2019.10.25**](#20191025)
  - [1. 论文阅读](#1-论文阅读)
  - [2. 机器学习](#2-机器学习)
    - [PCA与SVD关系 参考](#pca与svd关系-参考)
    - [Softmax和多个Logstic区别](#softmax和多个logstic区别)
    - [EM算法](#em算法)
    - [K means](#k-means)
  - [3. 概率论和矩阵](#3-概率论和矩阵)
    - [雅可比矩阵](#雅可比矩阵)
    - [贝叶斯派 vs 频率派](#贝叶斯派-vs-频率派)

## 1. 论文阅读

- [x] Mask R-CNN
- [x] FastSWA
  
## 2. 机器学习

### PCA与SVD关系 [参考](cnblogs.com/pinard/p/6239403.html)

**PCA**  
PCA属于**无监督降维**，找出数据里最主要成分去代替原始数据。具体的，设数据集是n维的，共有$m$个数据$(x(1),x(2),...,x(m))$。希望将这m个数据的维度从$n$维降到$k$维，并且新数据集尽可能的代表原始数据集。

方法  
基于最大投影方差或者最小投影距离

流程

- 数据归一化
- 计算样本协方差$XX^{T}$
- 求协方差特征值，取出前n个最大的特征向量并标准化，组成特征矩阵W
- 将原数据每一个样本通过W变换为新样本 $mn*nk = mk$

优点

- 仅以方差衡量信息量，不受其他因素影响
- 各主成分之间正交，消除原始数据成分间的相互影响的因素
- 主要运算是特征值分解，易于实现

缺点

- 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。
- 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

PCA与SVD的关系和区别

- SVD维度变换是 $mn = mm * mn * nn$，可以**两个方向上主成分**，而PCA只能获得单个方向上的主成分，而PCA仅用SVD的右奇异矩阵
- SVD左奇异矩阵可以用于行数的压缩，右奇异矩阵可以用于列数压缩
- SVD有一些实现算法可以不用先求出协方差矩阵，也能求出右奇异矩阵。也就是说，PCA可以不用做特征分解，而是做SVD来完成。这在样本量很大的时候很有效。

PCA可以用来降低过拟合么  
不适合，因为PCA是无监督降维，**没有考虑标签**，而且方差小的特征可能对数据有很大作用

### Softmax和多个Logstic区别

普通的logstic回归是二分类问题，而Softmax是多分类问题，要想实现多分类，需要改进logistic回归，具体做法是：

- 每个类别都建立一个二分类器，属于该类别的为1，其他为０
- 对于选择softmax还是k个logistics回归，取决于所有类别之间是否互斥。
  - 所有类别之间明显互斥用Softmax；
  - 所有类别之间不互斥有交叉的情况下最好用k个logistics分类器。

### EM算法

作用  

- 用于含有隐变量模型的参数极大似然估计
- $Y$是为观测随机变量（不完全数据），$Z$为隐随机变量，$Y+Z$是完全数据，算法目标就是极大化观测数据$Y$对于参数$θ$的似然函数

**Q函数**  
完全数据的对数似然函数在z的条件概率下的期望
$$
E_{q}(L_{c}(x_{i},z_{i};\theta)) = \sum_{k=1}^{K}L_{c}(x_{i},z_{i}=k;\theta)q(z_{i}=k|x_{i})
$$

算法

1. 初始化参数θ 
2. E step: 求当前参数下的Q函数
3. M step: 求使Q极大化的参数θ
4. 迭代2和3

### K means

目标函数  
把n个点划分到K个聚类中，使得每个点都属于离它最近点对应的聚类，以之作为聚类的标准，属于无监督，满足最小化质点和属于该中心的数据点之间的平方距离和（Sum of Square Distance）  

算法  

1. 选择K值，然后选择K个初始质点 $[C1,C2....Ck]$  
   这一步是关键，选择不好将落入局部最优解，传统方法是随机选K个点，改进是Kmeans++
2. 分配，将数据点分配给与之有最小MSE的点  
   对于任意一个点，遍历计算其与K个质点的MSE，选择最小MSE为分配点
3. 更新质点，求每个簇中数据点的均值，作为新的质点
4. 重复2和3步骤直到簇中点不再更新

优化

- Kmeans初值选取，手肘法，轮廓系数法
- 距离计算用elkan或Mini batch Kmeans

## 3. 概率论和矩阵

### 雅可比矩阵

- 雅可比矩阵是多元函数一阶偏导数的集合，对应一个**函数族**，有m个函数，每个函数有n个变量  

- 由于矩阵描述了向量空间中的运动——变换，而雅可比矩阵看作是将点{${x_{1},x_{2}...x_{n}}$}转化到点{${y_{1},y_{2}...y_{m}}$}，或者说是从一个n维的欧式空间转换到m维的欧氏空间

### 贝叶斯派 vs 频率派

- 概率和统计  
  **概率**是已知模型和参数，去预测结果，如选好品种、喂养方式、猪棚的设计等等（选择参数），想知道养出来的猪肉质怎么样（预测结果）  

  **统计**是已知数据，去推测模型和参数，买了一斤肉，根据肉质判断猪的品种，喂养方式等  
  一句话总结，概率是已知参数求结果，统计是已知结果求参数

- 先验分布 & 后验分布  
  **先验分布**是在取得实验观测值以前对一个参数概率分布的主观判断  

  **后验分布**是在取得实验数据下对参数概率分布的修正

- 频率派和贝叶斯派  
  频率派认为估计参数是一个**未知的固定值**，本着客观事实思想，从数学极限的角度出发，其重要支柱是大数定理，

  贝叶斯派认为估计参数都是**随机变量**，根据已有的经验和知识推断一个先验概率， 然后在新数据不断积累的情况下调整这个概率（后验概率）

- 最大似然和最大后验  
  频率学派 - 最大似然估计  

  - 翻译成最大可能性估计比较好，主要思想是求参数θ，使所有事件发生概率（似然函数值）最大
  - 利用最优化思想求解似然函数取max时的模型参数值
   $$
    L(\theta) = f(x_{1}|\theta)f(x_{2}|\theta)...f(x_{n}|\theta)
    $$
  - 针对$f(x|θ)$  
    如果$θ$确定，$x$是变量，这个函数叫做概率函数(probability function)，它描述不同样本点在参数$θ$下的$x$出现概率是多少  
    如果$x$确定，$θ$是变量，这个函数叫做似然函数(likelihood function)， 它描述不同参数$θ$下，某个样本点$x$出现概率是多少

  贝叶斯学派 - 最大后验估计
  - 就是最大化贝叶斯公式
  $$
    \hat{\theta}= argmaxP(\theta|X) \\
        = argmin -logP(X|\theta)-logP(\theta) 
  $$
  - 注意到MAP与MLE的区别在于MAP加入了先验项p(θ)，有种正则化的意味
  - 在MAP中使用一个高斯分布的先验等价于在MLE中采用L2正则
