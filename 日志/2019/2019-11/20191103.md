# 深度学习

## １．Normalization总结

**BN**
```python
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
```

- 每个channel上计算均值和方差，对NHW做归一化，并滑动平均保存，momentum=0.1
- 对每个channel C的归一化结果进行缩放和平移，设置affine=True，即weight(γ)和bias(β)将被使用，初始条件(γ)=1，(β)=0 ，**C个参数**
- track_running_stats=True追踪均值和方差

**LN**
```python
torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)
```

- 每个batch计算均值和方差，对HWC做归一化
- 对每个元素的归一化的结果进行缩放和平移，设置elementwise_affine=True，**HWC个参数**

**IN**
```python
torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
```

- batch上每个样本的channel计算均值和方差，对HW做归一化，并且不做缩放和平移

**GN**
```python
torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)
```

- batch内的每个样本，将channel C分组成G个，对每HWC//G做归一化，然后缩放和平移，**NC个参数**

**使用场景**

- 图像生成和风格迁移用IN （GAN）
- Batchsize大用BN，小用GN
- RNN用LN

## ２．困难样本发掘总结

见印象笔记