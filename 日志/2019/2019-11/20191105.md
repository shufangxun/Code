# 深度学习

## One-stage vs Two-stage

**One-stage**  
网络在原图上提出很多预先设定的粗略anchors，**数目很多**，然后利用回归loss来修正这些anchors

- Yolov1没有anchor且是全连接层，属于直接回归，效果一般
- Yolov2&v3利用anchor并移除全连接层，但是没有ROIpooling

**Two-stage**  
网络通过两阶段精修anchor，第一阶段前背景分类与回归，第二阶段做分类与回归

**两者优缺点分析**

- One-stage少了一个阶段，速度更快，但是由于：１）anchor数目太多并且正负样本不平衡　２) anchor和feature misalign，导致精度不够
- Two-stage两阶段精修，精度更高，但速度慢

**One stage 精度不高的问题本质**  

- misalign
参考aligndet reppoints Guide anchoring  
- 测试和训练的Gap [链接](https://www.zhihu.com/search?type=content&q=Consistent%20Optimization%20for%20Single-Shot%20Object%20Detection)
  1. 分类和回归是两个独立分支，分类是用原始anchor打标签，回归分支经过refine后预测位置相对原始anchor有所偏移，但是它的标签仍然是原始anchor的，这就导致misalign.  
  2. 举个例子: 两个anchor分别是A和B，对应bike，然后回归过程中，A没有B匹配bike效果好，但是根据NMS，保留的是A
- one-stage中预测出的位置相对anchor是有偏移量的，而类别置信度是针对原始位置的特征并不是偏移后的特征，提出解决办法
  [链接](https://ac.nowcoder.com/discuss/229537)

## Anchor

**Anchor box作用**  为什么要用Anchor?  
**不用anchor的缺点**

- 传统滑窗一个窗口只能检测一个目标，且无法解决多尺度问题
- 缺乏先验框，bbox回归范围比较大会让loss范围变化很大，而且对大小目标的精度有不同的偏好
- 固定框，不同尺寸目标都是依赖少数几个level的特征图预测

**用anchor的优点**
- anchor作用是显式的枚举出不同的scale和aspect ratio框，这样带来的优势：
  - 相当于加上了约束，给出了一个基准，网络学习的是相对anchor 的偏移
  - 把不同尺寸的目标分配到对应的anchor，这样对应anchor的网络负责一个比较小的目标尺寸范围，同时学习过程中，是显式的通过不同的分支传递不同尺寸目标的 loss，让网络更加符合“逻辑”的得到训练

**Anchor缺点** [参考](https://zhuanlan.zhihu.com/p/50816080)
- anchor数目过多，正负样本不均衡
- anchor超参过多
    - 尺寸和纵横比都是离散的，如何制定先验框困难
    - 从训练的角度来看，希望能有一个更加平滑的预测，从经验上看，可以增加网络的泛化性能，但是anchor是离散的，打破了平滑

**本质**
- anchor将问题转化为anchor中是否有目标，相对目标的偏移是多少，从而根据目标框修正anchor
- anchor让网络学习到一种推理能力，根据anchor所对应的感受野，判断anchor为目标的概率，anchor可能比感受野大，也可能比感受野小：
  - 如果比感受野大，相当于只看到了关心区域（anchor）的一部分（感受野），通过部分判断整体
  - 如果比感受野小，那就是看到了比关心区域更大的信息，判断关心的区域是不是目标。

所以感受野大于anchor比较好

**与传统目标检测区别**  
传统目标检测是金字塔+遍历滑窗，逐尺度逐位置判断有没有目标，anchor预设不同尺度和不同位置的参考框去覆盖所有位置，本质也是多尺度的滑窗，只是不需要遍历